{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pygrok import Grok\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import operator\n",
    "import pandas as pd\n",
    "import string\n",
    "import preprocessor as p\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp,os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Packages for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, nltk, gensim, spacy\n",
    "from textblob import TextBlob\n",
    "import pickle as p\n",
    "\n",
    "#### nltk tagger and punk download\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "### nltk packages\n",
    "from nltk.data import load\n",
    "from nltk.tokenize.casual import TweetTokenizer, casual_tokenize\n",
    "from nltk.tokenize.mwe import MWETokenizer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize.regexp import (\n",
    "    RegexpTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    "    BlanklineTokenizer,\n",
    "    WordPunctTokenizer,\n",
    "    wordpunct_tokenize,\n",
    "    regexp_tokenize,\n",
    "    blankline_tokenize,\n",
    ")\n",
    "from nltk.tokenize.repp import ReppTokenizer\n",
    "from nltk.tokenize.sexpr import SExprTokenizer, sexpr_tokenize\n",
    "from nltk.tokenize.simple import (\n",
    "    SpaceTokenizer,\n",
    "    TabTokenizer,\n",
    "    LineTokenizer,\n",
    "    line_tokenize,\n",
    ")\n",
    "from nltk.tokenize.texttiling import TextTilingTokenizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize.util import string_span_tokenize, regexp_span_tokenize\n",
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "import num2words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Grok for string matching and clustering OS Logs to pre-defined rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}avahi-autoipd.*Callout.*,%{SPACE}address%{SPACE}%{IP:src_ip_address}.*')# 0\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}avahi-autoipd.*Starting.*%{SPACE}address%{SPACE}%{IP:src_ip_address}.*')# 1\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}avahi-autoipd.*Successfully.*%{SPACE}address%{SPACE}%{IP:src_ip_address}.*')# 2\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Accepted%{SPACE}password%{SPACE}for%{SPACE}%{USER:local_user}%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 3\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Accepted%{SPACE}password%{SPACE}for%{SPACE}%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 4\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Accepted%{SPACE}publickey%{SPACE}for%{SPACE}%{USER:local_user}%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 5\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Accepted%{SPACE}publickey%{SPACE}for%{SPACE}%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 6\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Connection%{SPACE}closed%{SPACE}by%{SPACE}%{IP:src_ip_address}.*')# 7\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Connection%{SPACE}reset%{SPACE}by%{SPACE}%{IP:src_ip_address}.*')# 8\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Disconnected%{SPACE}from%{SPACE}%{IPORHOST:src_ip_address}.*')# 9\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*error:%{SPACE}maximum%{SPACE}authentication%{SPACE}attempts%{SPACE}exceeded%{SPACE}for%{SPACE}%{USER:local_user}%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 10\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*error:%{SPACE}maximum%{SPACE}authentication%{SPACE}attempts%{SPACE}exceeded%{SPACE}for%{SPACE}%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 11\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Failed%{SPACE}none%{SPACE}for%{SPACE}invalid%{SPACE}user%{SPACE}%{USER:local_user}%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 12\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Failed%{SPACE}none%{SPACE}for%{SPACE}invalid%{SPACE}user%{SPACE}%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 13\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Failed%{SPACE}password%{SPACE}for%{SPACE}%{USER:local_user}%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 14\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*Failed%{SPACE}password%{SPACE}for%{SPACE}%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 15\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*fatal:%{SPACE}packet_write_wait:%{SPACE}Connection%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 16\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*:%{SPACE}input_userauth_request: invalid%{SPACE}user%{SPACE}%{IP:src_ip_address}.*')# 17 \n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*:%{SPACE}PAM.*authentication%{SPACE}failure.*%{IP:src_ip_address}%{SPACE}%{USER:local_user}.*')# 18\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*:%{SPACE}PAM.*authentication%{SPACE}failure.*%{IP:src_ip_address}%{SPACE}.*')# 19\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*:%{SPACE}pam_unix\\(sshd:auth\\).*authentication%{SPACE}failure.*%{IP:src_ip_address}%{SPACE}%{USER:local_user}.*')# 20\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*:%{SPACE}pam_unix\\(sshd:auth\\).*authentication%{SPACE}failure.*%{IP:src_ip_address}%{SPACE}.*')# 21\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*:%{SPACE}Received%{SPACE}disconnect%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 22\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*fatal:%{SPACE}ssh_dispatch_run_fatal:%{SPACE}Connection%{SPACE}from%{SPACE}%{IP:src_ip_address}.*')# 23\n",
    "patterns.append('%{SYSLOGTIMESTAMP}%{SPACE}%{SYSLOGHOST}%{SPACE}.*fatal:%{SPACE}Unable%{SPACE}to%{SPACE}negotiate%{SPACE}with%{SPACE}%{IP:src_ip_address}.*') # 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/334968 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== RESULTS =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334968/334968 [3:55:29<00:00, 23.71it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(line, annotated_ips_l1):\n",
    "    annotation = dict()\n",
    "    for j, ptrn in enumerate(patterns):\n",
    "        gork_match = Grok(ptrn).match(line)\n",
    "        if (gork_match != None):\n",
    "            try: \n",
    "                annotation['local_user'] = gork_match['local_user'] \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            annotation['src_ip_address'] =  gork_match['src_ip_address'] \n",
    "            annotation['action'] = j\n",
    "            annotated_ips_l1.append(annotation)\n",
    "            break\n",
    "    \n",
    "    return annotated_ips_l1\n",
    "\n",
    "\n",
    "def process_wrapper(chunkStart, chunkSize,abs_file_path):\n",
    "    with open(abs_file_path) as f:\n",
    "        annotated_ips_l2=[]\n",
    "        annotated_ips_l1=[]\n",
    "        result_l2 = []\n",
    "        f.seek(chunkStart)\n",
    "        lines = f.read(chunkSize).splitlines()\n",
    "        for line in lines:\n",
    "            result_l2 = process(line,annotated_ips_l1)\n",
    "        #print(result_l2)\n",
    "        annotated_ips_l2.extend(result_l2)\n",
    "        return annotated_ips_l2\n",
    "\n",
    "\n",
    "def chunkify(fname,size=4096):\n",
    "    fileEnd = os.path.getsize(fname)\n",
    "    with open(fname,'rb') as f:\n",
    "        chunkEnd = f.tell()\n",
    "        while True:\n",
    "            chunkStart = chunkEnd\n",
    "            f.seek(size,1)\n",
    "            f.readline()\n",
    "            #print(var_1)\n",
    "            #print(var_2)\n",
    "            chunkEnd = f.tell()\n",
    "            #print(\"Chunk Start: %s Chunk End: %s Size: %s\" %(chunkStart,chunkEnd,chunkEnd-chunkStart))\n",
    "            yield chunkStart, chunkEnd - chunkStart\n",
    "            if chunkEnd > fileEnd:\n",
    "                break\n",
    "\n",
    "                \n",
    "cores=90\n",
    "\n",
    "pool = mp.Pool(cores)\n",
    "jobs = []\n",
    "\n",
    "abs_file_path = 'json_file_message.txt'\n",
    "#create jobs\n",
    "for chunkStart,chunkSize in chunkify(abs_file_path):\n",
    "    #temp_words = []\n",
    "    #print(\"Chunk-Start: %s Chunk-Size: %s\" %(chunkStart,chunkSize))\n",
    "    jobs.append( pool.apply_async(process_wrapper,(chunkStart,chunkSize,abs_file_path)) )\n",
    "\n",
    "\n",
    "print(\"============== RESULTS =============\")\n",
    "results=[]\n",
    "#wait for all jobs to finish\n",
    "for job in tqdm(jobs):\n",
    "    job.wait()\n",
    "    #print(len(job.get()))\n",
    "    job_result = job.get()\n",
    "    results.extend(job_result)\n",
    "    #print(len(results))\n",
    "\n",
    "\n",
    "#clean up\n",
    "pool.close()\n",
    "#pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10508887"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 5, 'local_user': 'student', 'src_ip_address': '10.1.1.54'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(\"logs_annotated_ip_usr.txt\", \"w\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    line = str(result)\n",
    "    outF.write(line)\n",
    "    outF.write(\"\\n\")\n",
    "\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs_annotated.json\", \"w\") as output_file:\n",
    "    json.dump(results, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(results, open('result.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_results_df = pd.DataFrame(results)\n",
    "my_results_df.to_csv('logs_annotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_index = my_results_df.groupby('src_ip_address').count().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "for ip in my_index:\n",
    "    if ip[:3] == '10.':\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['10.1.1.10', '10.1.1.11', '10.1.1.12', '10.1.1.13', '10.1.1.14',\n",
       "       '10.1.1.15', '10.1.1.16', '10.1.1.17', '10.1.1.18', '10.1.1.19',\n",
       "       '10.1.1.20', '10.1.1.21', '10.1.1.22', '10.1.1.23', '10.1.1.24',\n",
       "       '10.1.1.25', '10.1.1.26', '10.1.1.28', '10.1.1.29', '10.1.1.30',\n",
       "       '10.1.1.32', '10.1.1.33', '10.1.1.34', '10.1.1.35', '10.1.1.36',\n",
       "       '10.1.1.37', '10.1.1.38', '10.1.1.39', '10.1.1.4', '10.1.1.40',\n",
       "       '10.1.1.41', '10.1.1.42', '10.1.1.43', '10.1.1.44', '10.1.1.45',\n",
       "       '10.1.1.46', '10.1.1.47', '10.1.1.48', '10.1.1.49', '10.1.1.5',\n",
       "       '10.1.1.50', '10.1.1.51', '10.1.1.52', '10.1.1.53', '10.1.1.54',\n",
       "       '10.1.1.55', '10.1.1.56', '10.1.1.57', '10.1.1.58', '10.1.1.6',\n",
       "       '10.1.1.61', '10.1.1.64', '10.1.1.65', '10.1.1.7', '10.1.1.8',\n",
       "       '10.1.1.9'],\n",
       "      dtype='object', name='src_ip_address')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_index[:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
