{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "from csv import DictReader\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import operator\n",
    "import pandas as pd\n",
    "import string\n",
    "import preprocessor as p\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File_Names_List = ['wtmp.log', 'cups_access.log', 'lightdm_seat0-greeter.log', 'syslog.log', 'filebeat.log', 'fontconfig.log', 'lastlog.log', 'cups_page.log', 'cloud-init-output.log', 'packetbeat.log', 'ufw.log', 'alternatives.log', 'kern.log', 'lightdm_x-0.log', 'auth_temp.log', 'cups_error.log', 'apt_history.log', 'gpu-manager.log', 'dpkg.log', 'fsck_checkroot.log', 'cloud-init.log', 'auth.log', 'btmp.log', 'apport.log', 'Xorg.0.log', 'lightdm_lightdm.log', 'apt_term.log', 'unattended-upgrades-shutdown.log', 'unattended-upgrades.log', 'dhcp_hostname.log', 'fsck_checkfs.log']\n",
    "File_Names_List = [\"auth.log\",\"kern.log\",\"syslog.log\"] \n",
    "#File_Names_List = [\"cups_access.log\"]\n",
    "#File_Names_List = [\"kern.log\"]\n",
    "###import glob, os\n",
    "###File_Names_List = []\n",
    "###os.chdir(\"/home/ubuntu/logs/\")\n",
    "###for file in glob.glob(\"*.log\"):\n",
    "###    File_Names_List.append(file)\n",
    "###print(File_Names_List)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"/home/ubuntu/logs/cups_access.log\"\n",
    "def process(line,temp_words_l1):\n",
    "    temp_words_l0=[]\n",
    "    cnt = 0\n",
    "    for term in re.findall(r\"(?:[a-z][a-z'\\-_]+[a-z])\",line.lower()):\n",
    "        cnt += 1\n",
    "        if cnt < 3:\n",
    "            continue\n",
    "        else:\n",
    "            temp_words_l0.append(term)\n",
    "    \n",
    "    temp_words_l1.append(temp_words_l0)\n",
    "    #print(temp_words_l1)\n",
    "    return temp_words_l1\n",
    "\n",
    "def process_wrapper(chunkStart, chunkSize,abs_file_path):\n",
    "    with open(abs_file_path) as f:\n",
    "        temp_words_l2=[]\n",
    "        temp_words_l1=[]\n",
    "        result_l2 = []\n",
    "        f.seek(chunkStart)\n",
    "        lines = f.read(chunkSize).splitlines()\n",
    "        for line in lines:\n",
    "            result_l2 = process(line,temp_words_l1)\n",
    "        #print(result_l2)\n",
    "        temp_words_l2.extend(result_l2)\n",
    "        return temp_words_l2\n",
    "\n",
    "def chunkify(fname,size=4096):\n",
    "    fileEnd = os.path.getsize(fname)\n",
    "    with open(fname,'rb') as f:\n",
    "        chunkEnd = f.tell()\n",
    "        while True:\n",
    "            chunkStart = chunkEnd\n",
    "            f.seek(size,1)\n",
    "            f.readline()\n",
    "            #print(var_1)\n",
    "            #print(var_2)\n",
    "            chunkEnd = f.tell()\n",
    "            #print(\"Chunk Start: %s Chunk End: %s Size: %s\" %(chunkStart,chunkEnd,chunkEnd-chunkStart))\n",
    "            yield chunkStart, chunkEnd - chunkStart\n",
    "            if chunkEnd > fileEnd:\n",
    "                break\n",
    "\n",
    "                \n",
    "cores=90\n",
    "#bag_of_words = []\n",
    "file_dir=\"/home/ubuntu/logs/\"\n",
    "\n",
    "#init objects\n",
    "pool = mp.Pool(cores)\n",
    "jobs = []\n",
    "\n",
    "for file in File_Names_List:\n",
    "    abs_file_path = os.path.join(file_dir, file)\n",
    "    #create jobs\n",
    "    for chunkStart,chunkSize in chunkify(abs_file_path):\n",
    "        #temp_words = []\n",
    "        #print(\"Chunk-Start: %s Chunk-Size: %s\" %(chunkStart,chunkSize))\n",
    "        jobs.append( pool.apply_async(process_wrapper,(chunkStart,chunkSize,abs_file_path)) )\n",
    "#        bag_of_words.append(temp_words)\n",
    "\n",
    "# Get process results from the output queue\n",
    "#results = [job.get() for job in jobs]\n",
    "\n",
    "print(\"============== RESULTS =============\")\n",
    "results=[]\n",
    "#wait for all jobs to finish\n",
    "for job in jobs:\n",
    "    job.wait()\n",
    "    #print(len(job.get()))\n",
    "    job_result = job.get()\n",
    "    results.extend(job_result)\n",
    "    #print(len(results))\n",
    "\n",
    "\n",
    "#clean up\n",
    "pool.close()\n",
    "#pool.join()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for job in jobs:\n",
    "    counter+=1\n",
    "print(counter)\n",
    "print(len(results))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results_rev1.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "    \n",
    "#vocabulary_to_load = pickle.load(open(dictionary_filepath, 'r')) \n",
    "#loaded_vectorizer = sklearn.feature_extraction.text.countvectorizer(ngram_range=(ngram_size,ngram_size), \n",
    "#min_df=1, vocabulary=vocabulary_to_load) \n",
    "#loaded_vectorizer._validate_vocabulary() \n",
    "#print('loaded_vectorizer.get_feature_names(): {0}'.   \n",
    "#format(loaded_vectorizer.get_feature_names())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5f43550a7f06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results_rev1.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mobject_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "file = open(\"results_rev1.pkl\",'rb')\n",
    "object_file = pickle.load(file)\n",
    "\n",
    "results = []\n",
    "for item in object_file:\n",
    "    results.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=1)\n",
    "[' '.join(result) for result in results[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words=\"english\", analyzer='word', ngram_range=(1, 1),\n",
    "vect = CountVectorizer(max_df=1.0, min_df=1, max_features=20)\n",
    "X = vect.fit_transform([' '.join(result) for result in results[0:10]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X.toarray()))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_str = [\n",
    "#    r'<[^>]+>', # HTML tags\n",
    "#    r'(?:@[\\w_]+)', # @-mentions\n",
    "#    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "#    r'http[s]?://(?:[a-z][$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "#    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "#    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "#    r'(?:[\\w_]+)', # other words\n",
    "#    r'(?:\\S)' # anything else\n",
    "    r'\\w+'\n",
    "]\n",
    "#words = re.findall(r'\\w+', open('hamlet.txt').read().lower())\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize_1(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def tokenize_2(sent):\n",
    "    return [x.strip() for x in re.split(r\"(?:[a-z][a-z_]+[a-z])\", sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_results = DataFrame(X, columns=vect.get_feature_names())\n",
    "print (count_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ary = count_results.values\n",
    "print (count_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation= 1-pairwise_distances(count_ary, metric='cosine')\n",
    "print (correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlation, vmin=0, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, stop_words=\"english\", \n",
    "                    analyzer='word', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=20)\n",
    "Xi = f.fit_transform([' '.join(result) for result in results[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_df = DataFrame(Xi.A, columns=f.get_feature_names())\n",
    "print (tfid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_ary = tfid_df.values\n",
    "print (tfid_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation= 1-pairwise_distances(tfid_ary, metric='cosine')\n",
    "print (correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlation, vmin=0, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
